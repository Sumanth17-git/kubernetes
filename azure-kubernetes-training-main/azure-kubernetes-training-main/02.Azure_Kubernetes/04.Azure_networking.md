---

## ğŸŒ Azure CNI vs Azure CNI Overlay Networking in AKS

Kubernetes networking in AKS can be configured using either **Azure CNI (Classic)** or **Azure CNI Overlay**. Understanding the difference is critical for scaling, IP management, and connectivity with Azure services.

---

### ğŸ”¹ Azure CNI (Classic / VNet-Integrated)

- ğŸŸ¢ **Pods receive IPs directly from the Azure VNet subnet**.
- ğŸŸ¢ Each pod is **addressable within the VNet** (first-class IP).
- âœ… Pods can communicate **natively with Azure resources** (e.g., VMs, databases).
- â— Can exhaust IPs quickly in large-scale deployments.



---

### âœ… Azure CNI Overlay (Recommended for Large-Scale / Virtual Nodes)

- ğŸš« Pods **do not** get IPs from VNet subnet.
- ğŸŸ¢ Nodes still get IPs from VNet (e.g., 10.10.0.0/16).
- ğŸŸ¢ Pods are assigned IPs from a **separate overlay CIDR** (e.g., 192.168.0.0/16).
- ğŸ” **VXLAN tunneling** is used to route pod-to-pod traffic.
- ğŸŒ Outbound pod traffic is **NATed via the node's IP**.
- ğŸ§  Saves subnet IPs and supports massive scale.



> ğŸ“ Overlay CIDR is specified during cluster creation (e.g., `--pod-cidr 100.64.0.0/16`)

---

## ğŸ’¡ Example Comparison

| Feature                     | Azure CNI (Classic)      | Azure CNI Overlay             |
|----------------------------|--------------------------|-------------------------------|
| Pod IP Source              | Azure VNet subnet        | Separate overlay CIDR         |
| Pod-to-Azure Services      | Direct (native routing)  | NATed through node            |
| Pod-to-Pod Communication   | Native within subnet     | VXLAN encapsulated            |
| IP Consumption             | High                     | Low                           |
| Scale Potential            | Limited by subnet size   | Massive (>100K pods/cluster)  |
| Virtual Nodes Compatibility| âŒ No                    | âœ… Yes                         |

---

## âš¡ Virtual Nodes & Azure CNI Overlay

When you enable **Virtual Nodes**, AKS automatically switches to **Azure CNI Overlay** networking.

### ğŸ§  Why?

- Virtual Nodes run pods in **Azure Container Instances (ACI)** â€” a serverless environment.
- These pods are **not part of your VM node pool**.
- Azure CNI Overlay allows **pod burst capacity** without needing IPs from the core subnet.

### ğŸ“¦ What Happens When You Enable Virtual Nodes?

| Action                                | Result                                     |
|---------------------------------------|--------------------------------------------|
| Enable Virtual Nodes in AKS           | A **Virtual Kubelet** node is created      |
| Schedule extra pods to this node      | Pods run in **ACI**, not VM-based nodes    |
| Overlay networking used automatically | Pods get IPs from overlay CIDR             |
| ACI handles the compute & network     | You only pay per second used               |

---

## âœ… Summary

| Use Case                         | Recommended Network Plugin |
|----------------------------------|----------------------------|
| High-scale deployments (>1K pods)| Azure CNI Overlay          |
| VM-only workloads, small scale   | Azure CNI Classic          |
| Using Virtual Nodes / ACI        | Azure CNI Overlay (Required) |

> ğŸ“Œ Azure CNI Overlay is ideal for scaling pods while conserving IPs and enabling features like **Virtual Nodes**, **burstable compute**, and **hybrid networking**.

---

---

## ğŸ’¡ Example: Azure CNI vs Azure CNI Overlay with Virtual Nodes

Understanding how pod IP allocation and networking work is essential for scaling and secure connectivity in AKS.

---

### ğŸ“˜ CNI Classic Example

If you create an AKS cluster with **Azure CNI (Classic)**:

- VNet/Subnet: `10.240.0.0/16`
- Pod IPs are assigned directly from the subnet


âœ… Pods can **talk directly** to services in the VNet (SQL, Redis, VMs)

---

### âœ… CNI Overlay Example

If using **Azure CNI Overlay**:

- Node IP: from VNet subnet (e.g., `10.240.0.4`)
- Pod IP: from **overlay CIDR** (e.g., `192.168.0.5`, `192.168.0.6`)
- Overlay range: `--pod-cidr 192.168.0.0/16`

ğŸ” Traffic between pods is encapsulated with **VXLAN**  
ğŸŒ Outbound internet traffic is **NATed** through node IP  
ğŸš« Azure services **cannot directly** reach pod IPs unless **Private Endpoints/NAT rules** are configured

---

## ğŸš€ What Happens When You Enable Virtual Nodes?

- Virtual Nodes use **Azure Container Instances (ACI)**
- Let you **burst workloads** instantly (no VM provisioning)
- **Overlay networking is required** and enabled by default

---

## ğŸ”§ Step-by-Step: Create AKS Cluster with Azure CNI Overlay

```bash
az aks create \
  --name aks-overlay-demo \
  --resource-group myrg \
  --network-plugin azure \
  --network-plugin-mode overlay \
  --pod-cidr 192.168.0.0/16 \
  --service-cidr 10.0.0.0/16 \
  --dns-service-ip 10.0.0.10 \
  --vnet-subnet-id /subscriptions/<SUB-ID>/resourceGroups/myrg/providers/Microsoft.Network/virtualNetworks/myVnet/subnets/aks-subnet \
  --generate-ssh-keys

kubectl run nginx --image=nginx
kubectl get pod -o wide
kubectl exec -it nginx -- curl google.com
kubectl exec -it nginx -- ping <azure-sql-private-ip>
```
# ğŸŒ Kubernetes Networking & Service Discovery â€” Beginner to Pro

---

## ğŸ§± 1. Kubernetes Networking Model (Flat Network)

Kubernetes networking is the system that allows **Pods to talk to each other** and to the **outside world**.

### ğŸ”‘ Core Rules of Kubernetes Networking:

1. ğŸ“¦ Each **Pod gets a unique IP address**
2. ğŸ”„ All Pods can **communicate directly** with each other â€” **no NAT required**
3. ğŸ§­ Communication is **flat** â€” no gateways or routers between Pods
4. ğŸ§µ **Containers in the same Pod** share the same network namespace (can use `localhost`)

---

## ğŸ“š Topics Covered:

- âœ… What is a **Virtual Network (VNet)** in AKS
- âœ… What is a **subnet** and how `/16`, `/24` ranges work
- âœ… What are **Pod CIDR**, **Service CIDR**, and how Azure assigns them
- âœ… What is **DNS IP** and how **CoreDNS** works
- âœ… How all this relates to your **AKS VNet configuration**

---

## ğŸ¢ Real-Life Analogy: Office Building

Think of a **Virtual Network (VNet)** as an office building in a corporate environment:

| Element        | Real World Analogy                 |
|----------------|------------------------------------|
| VNet           | Office Building                    |
| Subnet         | Rooms inside the office            |
| VMs / Nodes    | Employees in rooms                 |
| NSG / Firewall | Security Guards                    |

- Everyone inside (pods/VMs) can **communicate privately**
- **No need to go through the internet**
- Guards (**NSGs**) decide **who can enter or exit**

---

## âœ… Example Use Case: AKS + Azure SQL DB

Scenario:
- You have an AKS cluster in `aks-subnet`
- Azure SQL is in `sql-subnet`
- Both subnets are within the same VNet

ğŸ’¡ Want to ensure **secure, internal communication**?

### âœ”ï¸ Solution:
- Place both AKS and SQL **inside the same VNet**
- Communication happens **privately** using **private IPs**
- No public exposure, **no internet routing involved**

```plaintext
[VNet: company-network]
    â”œâ”€â”€ Subnet: aks-subnet â†’ AKS Cluster
    â””â”€â”€ Subnet: sql-subnet â†’ Azure SQL DB

â†’ Private IP to Private IP
â†’ Fast, secure, no egress charges
```

# ğŸŒ Kubernetes Networking & Service Discovery â€” Beginner to Pro (Part 2â€“4)

---

## ğŸ”— PART 2: What is a Subnet?

### âœ… Basic Definition:
A **subnet** is a smaller, logical subdivision of a **Virtual Network (VNet)**. It allows better organization and isolation of networked resources.

### ğŸ§± Subnet Examples in an AKS Architecture:

| Subnet Name          | Purpose                              |
|----------------------|--------------------------------------|
| `aks-subnet`         | Hosts AKS nodes and Pod IPs          |
| `appgateway-subnet`  | Hosts Azure Application Gateway      |
| `database-subnet`    | Hosts Azure SQL / Database resources |

ğŸ’¡ Subnets help in applying **NSGs**, **routing rules**, and **segregating workloads** cleanly inside a VNet.

---

## ğŸ“ PART 3: What is CIDR and /16, /24, /12?

### âœ… CIDR = Classless Inter-Domain Routing

CIDR determines how **many IP addresses** are available in your network/subnet.

| CIDR Notation | IP Range Size     | Example IP Range                     |
|---------------|-------------------|--------------------------------------|
| `/24`         | 256 IPs           | `10.0.0.0` â†’ `10.0.0.255`            |
| `/16`         | 65,536 IPs        | `10.0.0.0` â†’ `10.0.255.255`          |
| `/12`         | ~1 Million IPs    | `10.0.0.0` â†’ `10.15.255.255`         |

> ğŸ§  **Why It Matters?**
> - AKS uses IPs for **VMs**, **Pods**, and **Services**
> - If you choose a **small CIDR** (e.g., `/24`), your cluster may **run out of IPs** quickly.

---

## ğŸ³ PART 4: How Does This Connect to AKS?

When you deploy an AKS cluster, several **network-related resources** come into play:

### ğŸ”§ AKS Needs:

- âœ… A **subnet** to host the **worker nodes (VMs)**
- âœ… A **CIDR range** to assign IPs to **Pods** (`--pod-cidr`)
- âœ… A **CIDR range** to assign IPs to **Services** (`--service-cidr`)
- âœ… **DNS Service IP** (e.g., `10.0.0.10`) â€” used by **CoreDNS**
- âœ… Optional: Subnet for **Ingress Controllers** (e.g., App Gateway)

---

## ğŸ“Œ Real AKS Network Parameter Example

```bash
az aks create \
  --name aks-network-demo \
  --resource-group rg-aks \
  --network-plugin azure \
  --vnet-subnet-id "/subscriptions/xxx/resourceGroups/rg-aks/providers/Microsoft.Network/virtualNetworks/vnet-demo/subnets/aks-subnet" \
  --pod-cidr 192.168.0.0/16 \
  --service-cidr 10.0.0.0/16 \
  --dns-service-ip 10.0.0.10 \
  --generate-ssh-keys
```

# ğŸ“¡ Pod-to-Pod Communication in Kubernetes

This guide demonstrates how two applications (pods) in a Kubernetes cluster communicate **internally**, without exposing any services externally.

---

## âœ… Pod-to-Pod Communication (Flat Network Model)

In Kubernetes (including AKS), pods can talk to each other **directly** using their internal IP addresses.

### ğŸ” Key Concepts:
- Each pod gets a **unique IP address** from the **Pod CIDR** range.
- Kubernetes provides a **flat, routable network**:
  - Pods across **nodes** and **namespaces** can communicate **without NAT**.
- No need to expose apps externally (e.g., LoadBalancer, NodePort) for **internal communication**.

---

## ğŸ§ª Step-by-Step Demo: Frontend Pod â†’ Backend Pod Communication

---

### 1ï¸âƒ£ Create a Namespace

```bash
kubectl create namespace demo-ip
```
### 2ï¸âƒ£ Deploy a Backend Pod That Responds to HTTP
``` bash
# backend-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: backend
  namespace: demo-ip
  labels:
    app: backend
spec:
  containers:
  - name: backend
    image: hashicorp/http-echo
    args:
      - "-text=Hello from Backend via IP"
    ports:
    - containerPort: 5678
```


---

### âœ… 3ï¸âƒ£ Get Backend Pod IP

Once the backend pod is running, retrieve its internal IP address:

```bash
kubectl get pod -n demo-ip -o wide

## âœ… 4ï¸âƒ£ Deploy a Frontend Pod to Curl the Backend Pod

Now letâ€™s deploy a `curl`-based Pod that will attempt to connect to the backend using its **Pod IP**.

---

### ğŸ“„ `frontend-curl.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  namespace: demo-ip
spec:
  containers:
  - name: curl
    image: curlimages/curl
    command: [ "sleep", "3600" ]
```
## ğŸš€ Apply the YAML

```bash
kubectl apply -f frontend-curl.yaml
```bash
kubectl exec -n demo-ip -it frontend -- sh
curl http://<backend_pod_ip>:5678

#### âœ… Expected Output
Hello from Backend via IP

# AKS Networking: Virtual Network, Subnet, Pod CIDR, and Service CIDR

---

## ğŸ”¹ Virtual Network / Subnet Address Space

- To check the nodes and their IPs in your AKS cluster:

```bash
kubectl get nodes -o wide
```
### Example Node IP

`10.22.0.4`

---

### How to Check Virtual Network and Subnet Range in Azure Portal

1. Navigate to your **Virtual Network** resource.

2. Click on **Settings** â†’ **Subnets**.

3. View the subnet ranges and connected devices.


# ğŸšª What is Pod Isolation?

Pod isolation means controlling which Pods can communicate with each other.

In a default Kubernetes setup:
- All pods can talk to all other pods, across all namespaces.
- This is not secure by default (no zero-trust).
- Pod isolation is achieved using **Network Policies**.

---

## ğŸ§© Two Types of Isolation

| Type    | Controls...                | Enabled via                   |
|---------|----------------------------|------------------------------|
| ğŸ” Ingress | Who can talk to a pod      | `ingress` in a NetworkPolicy  |
| ğŸš« Egress  | Who a pod can talk to      | `egress` in a NetworkPolicy   |

---

## ğŸ§ª How Does Pod Isolation Work?

Pod isolation is **not automatic**.

A pod becomes **"isolated"** only when a matching NetworkPolicy exists that:
- Selects the pod, and
- Defines allowed traffic.

---

## ğŸš¦ What is a Network Policy?

Think of it like a firewall for Pods â€” it controls who can talk to whom inside your Kubernetes cluster.

---

## ğŸ¯ Why Do You Need Network Policies?

Without network policy:

- âœ… All Pods can talk to all other Pods.
- âŒ Thereâ€™s no isolation between frontend, backend, database, etc.
- âŒ Security risk if one pod is hacked â€” it can reach everything!

With network policy:

- ğŸ” You can allow only specific Pods or Namespaces to talk.
- ğŸ”’ You can block egress to the internet or IP ranges.
- ğŸ‘® You define **â€œwho can talk to whomâ€** inside your cluster.


| Mode   | Enforced by     | Supports Ingress | Supports Egress | Supports IPBlock | Use Case           |
|--------|-----------------|------------------|-----------------|------------------|--------------------|
| None   | âŒ No enforcement | âŒ No restrictions | âŒ No restrictions | âŒ No rules       | Dev/test clusters   |
| Azure  | âœ… Azure NSG     | âœ… Yes           | âŒ No           | âŒ No            | Basic secure apps   |
| Calico | âœ… Calico agent  | âœ… Yes           | âœ… Yes          | âœ… Yes           | Secure enterprise apps |


## ğŸ” Letâ€™s Break It Down One-by-One

### ğŸ›‘ 1. Network Policy: None
This means no rules, no restrictions, and no enforcement.

**ğŸ”“ Example:**
- Your frontend, backend, and database Pods can freely communicate with each other.
- Even if you write a NetworkPolicy YAML â€” it will be ignored!

**ğŸ§ª Use case:**
- For quick testing, lab clusters, or where security is not a concern.

**ğŸ“Œ Azure default when you don't specify `--network-policy`.**

---

### ğŸ›¡ï¸ 2. Network Policy: Azure
Enforced using Azure NSGs (Network Security Groups) under the hood.

**âœ… Supports:**
- Ingress (Who can send traffic to a Pod)

**âŒ Does NOT support:**
- Egress rules (You cannot restrict outgoing connections)
- IPBlocks (CIDRs)

**ğŸ“¦ Azure mode is good when:**
- You want basic intra-cluster security
- You want to block Pod-to-Pod communication except for allowed ones

---

### ğŸ§± 3. Network Policy: Calico
Uses Calico plugin to enforce rules.

**âœ… Supports:**
- Ingress
- Egress
- IP CIDR (IPBlock)
- Namespace selectors

This is full-featured, open-source, and CNCF compliant.

---

### ğŸ”¹ 1. Flat Networking Model
- Every Pod gets its own IP address.
- All Pods are in the same flat networkâ€”no NAT is needed.
- This makes it easy to treat a Pod like a mini server.
## ğŸ“¦ Key Rule
All Pods can talk to all other Pods in the cluster by default, across nodes and namespaces.

---

## ğŸ”¹ 2. Pod-to-Pod Communication
Kubernetes uses CNI plugins like:
- **Azure CNI (AKS default):** Pods get IPs from Azure VNet.
- **Kubenet (legacy):** Pods get internal IPs NATed via nodes.

### ğŸ“ Example:
If Pod A has IP `10.240.0.12`, and Pod B has `10.240.0.13`, they can directly curl each other.

---

## ğŸ”¹ 4. Ingress and Egress
- **Ingress:** Traffic coming into the cluster from outside.  
  Handled via LoadBalancers, Ingress Controllers (like NGINX, AGIC).
- **Egress:** Traffic going out to the internet or other networks.  
  Can be unrestricted, or controlled via Network Policy or firewall.

---

## ğŸ” Kubernetes Network Policies â€” for Isolation

### ğŸ”’ Without policies:
Everything can talk to everything = ğŸ§¨ **not secure!**

### âœ… Why Use Network Policies?
- Prevent Pod A from accessing Pod B unless allowed
- Lock down database access to backend only
- Prevent apps from making outbound internet calls
- Ensure frontend canâ€™t talk to another teamâ€™s namespace

---

## ğŸ§± How Network Policies Work
They work like firewalls at the Pod level.

- Based on:
  - Pod labels
  - Namespace selectors
  - IP blocks

If you apply any network policy to a Pod, Kubernetes **denies all traffic by default** â€” and only allows whatâ€™s explicitly defined.

In the diagram above you can see we have a **test-service** which requires accepting TCP connections **only from `client-green` pod** and **not from `client-red`**. In such a situation, a **NetworkPolicy** can help you control this traffic flow as desired.

---

### â— Can You Enforce Network Policies on a Cluster Created with `--network-policy none`?

âŒ **No**, you cannot enforce network policies on a cluster with `network-policy=none`.

Even if you apply a NetworkPolicy YAML, it will **not** be enforced because the cluster is **not configured with a network policy engine**.

---

### ğŸ” Why?

When you create an AKS cluster with:
--network-policy none
â€¢	No network policy plugin (like Azure or Calico) is installed.
â€¢	Kubernetes will accept your NetworkPolicy YAML (no errors), but will not enforce it.
â€¢	All pods can freely communicate, regardless of any rules written.
âœ… The API server stores the policy, but no CNI plugin applies it.

```bash
--network-policy none



az aks create \
  --resource-group sumi-testing \
  --name aks-none-demo \
  --location eastus \
  --node-count 2 \
  --node-vm-size Standard_B1ms \
  --enable-managed-identity \
  --network-plugin azure \
  --network-policy none \
  --generate-ssh-keys


```

# ğŸš€ PHASE 2: AKS with `network-policy=azure`

## ğŸ§  What is Azure Network Policy?

- Built-in policy support in AKS when using Azure CNI.
- Supports **only Ingress** rules (Egress blocking is **not supported**).
- Integrated with Azure NSGs and enforces policies at the VM level.
- Useful for internal microservices isolation.

## ğŸ¯ Real-Time Scenario

You are deploying a frontend and backend microservice. You want to:

1. âœ… Allow frontend to access backend  
2. ğŸš« Deny everything else  
3. âœ… Allow backend to talk to external services (Egress is always allowed with Azure policy)

```bash
az aks create \
  --resource-group sumi-testing \
  --name netpol-azure-cluster \
  --network-plugin azure \
  --network-policy azure \
  --node-vm-size Standard_B2s \
  --node-count 2 \
  --enable-managed-identity \
  --generate-ssh-keys

az aks get-credentials \
  --resource-group netpol-azure-rg \
  --name netpol-azure-cluster \
  --overwrite-existing
```

## Step 3: Deploy Frontend and Backend Pods

### ğŸ§© backend.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: backend
  labels:
    app: backend
spec:
  containers:
  - name: backend
    image: nginx
    ports:
    - containerPort: 80
```
ğŸ§© frontend.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    app: frontend
spec:
  containers:
  - name: curlbox
    image: radial/busyboxplus:curl
    command: ["sleep", "3600"]
```
```bash
kubectl apply -f backend.yaml
kubectl apply -f frontend.yaml
```
### ğŸ”¹ Step 4: Test Default Communication (Pre-Policy)

```bash
kubectl exec -it frontend -- curl <ip address backend>
```
âœ… Output: HTML from Nginx (success)
ğŸ§  All pods can talk by default.

### ğŸ”¹ Step 5: Create a Deny-All Ingress Policy

#### deny-all-ingress.yaml

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress

```bash
kubectl apply -f deny-all-ingress.yaml
kubectl exec -it frontend -- curl backend
```

âŒ Output: Connection refused â€” because ingress is denied for all pods.

---

### ğŸ”¹ Step 6: Allow Frontend â†’ Backend Only

#### allow-frontend-to-backend.yaml

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-backend
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
  policyTypes:
  - Ingress
```
```bash

kubectl apply -f allow-frontend-to-backend.yaml
kubectl get pods -o wide
kubectl exec -it frontend -- curl <backend ip address>
```

âœ… Output: Nginx page (Success)

---

### ğŸ”¹ Step 7: Confirm Other Pods Are Still Blocked

Create a third pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: attacker
  labels:
    app: attacker
spec:
  containers:
  - name: curlbox
    image: radial/busyboxplus:curl
    command: ["sleep", "3600"]
```
```bash
kubectl apply -f attacker.yaml
Test connectivity from attacker pod to backend:
kubectl exec -it attacker -- curl backend
âŒ Connection refused â€” since attacker is not allowed.
If you want, I can help with the next steps or explanation!
```

### ğŸ” Real-Time Use Cases

| Use Case                 | Azure Network Policy                   |
|--------------------------|--------------------------------------|
| âœ… Frontend â†” Backend     | Allowed (via label selector)          |
| ğŸš« Unknown Pod â†” Backend  | Denied                              |
| âœ… Backend to Internet    | Allowed (Egress is not enforced)      |
| ğŸš« DNS to kube-system     | Always allowed by AKS                  |

---

### ğŸ§± Limitations

- âŒ No Egress enforcement  
- âŒ No CIDR/IPBlock support  
- âŒ No namespace-level rules  

## ğŸš€ PHASE 3: Calico Mode â€” Advanced Network Policies in AKS

Calico is the most advanced and complete network policy implementation in Kubernetes.

We'll now use **Calico Network Policies** on Azure Kubernetes Service (AKS), covering real-time enterprise use cases with both **Ingress** and **Egress** controls.

---

### ğŸ“Š Feature Comparison: Azure Policy vs Calico

| Feature                    | Azure Policy | Calico |
|----------------------------|--------------|--------|
| âœ… Ingress filtering       | Yes          | Yes    |
| âœ… Egress filtering        | âŒ No        | âœ… Yes |
| âœ… IPBlock/CIDR rules      | âŒ No        | âœ… Yes |
| âœ… DNS-based egress control| âŒ No        | âš ï¸ Limited (requires sidecar/proxy) |
| âœ… Namespaced isolation    | Limited      | Yes    |
| âœ… Label-based enforcement | Yes          | Yes    |

---

### ğŸ¯ Real-Time Enterprise Scenario

Letâ€™s build **Calico Network Policies** from scratch with a complete, beginner-friendly, and step-by-step guide. This includes:

- ğŸš§ Default deny for all ingress and egress
- ğŸ”“ Allow internal service-to-service traffic (e.g., frontend â†’ backend)
- ğŸŒ Allow egress to internet for selected pods (e.g., curl google.com)
- ğŸ” Restrict based on namespaces or labels
- ğŸŒ Allow specific CIDR IPBlock traffic
- ğŸ“¦ Policy chaining and validation

---

> ğŸ”§ We'll use `kubectl`, YAML manifests, and live testing commands to validate the effect of each rule. Ideal for DevOps, SREs, and Trainers explaining network security in Kubernetes using Calico.

---

âœ… Ready to continue? Let me know if you'd like the first step: "Create a namespace and default deny policy".
```yaml
az aks create \
  --resource-group calico-lab-rg \
  --name calico-aks-demo \
  --node-count 2 \
  --node-vm-size Standard_B2s \
  --enable-managed-identity \
  --network-plugin azure \
  --network-policy calico \
  --generate-ssh-keys

```
## ğŸ”¥ Kubernetes NetworkPolicy: Phase 1 â€” Setup

A `NetworkPolicy` in Kubernetes is like a **firewall rule for Pods** â€” it controls:

- ğŸ” **Ingress**: Who can talk **to** your Pod
- ğŸ” **Egress**: Who your Pod can talk **to**

---

## ğŸ§ª Phase 1: Setup â€” Namespaces & Pods

### 1ï¸âƒ£ Create Namespaces

```bash
kubectl create namespace team-api
kubectl create namespace team-ui
kubectl create namespace testers

ğŸ”– Add Namespace Labels (needed for network policy namespaceSelector)
kubectl label ns team-api name=team-api
kubectl label ns team-ui name=team-ui
kubectl label ns testers name=testers
```yaml
Create Pods in Each Namespace
âœ… checkout.yaml (in team-api)
apiVersion: v1
kind: Pod
metadata:
  name: checkout
  namespace: team-api
  labels:
    role: checkout
spec:
  containers:
  - name: web
    image: nginx
    ports:
    - containerPort: 80
âœ… frontend.yaml (in team-ui)

apiVersion: v1
kind: Pod
metadata:
  name: frontend
  namespace: team-ui
  labels:
    role: frontend
spec:
  containers:
  - name: curl
    image: radial/busyboxplus:curl
    command: ["sleep", "3600"]

âœ… tester.yaml (in testers)

apiVersion: v1
kind: Pod
metadata:
  name: tester
  namespace: testers
  labels:
    role: tester
spec:
  containers:
  - name: curl
    image: radial/busyboxplus:curl
    command: ["sleep", "3600"]
```
```bash
kubectl apply -f checkout.yaml
kubectl apply -f frontend.yaml
kubectl apply -f tester.yaml

```
## ğŸ” Phase 2: Baseline Test â€” Everything Should Work

Before applying any NetworkPolicy, Kubernetes allows **all pods to communicate** with each other â€” across namespaces and to the Internet.

Weâ€™ll validate this default behavior.

---

### âœ… 1. Frontend â†’ Checkout

```bash
kubectl exec -n team-ui frontend -- curl -s checkout.team-api.svc.cluster.local
```
### âœ… 2. Tester â†’ Checkout

```bash
kubectl exec -n testers tester -- curl -s checkout.team-api.svc.cluster.local
```
Expected: âœ… Successful response from the checkout service.

### âœ… 3. Checkout â†’ Internet
```bash
kubectl exec -n team-api checkout -- curl -s https://ifconfig.me

All these should work before we apply any policies.
```
## ğŸ”’ Phase 3: Use Case 1 â€” Deny All in `team-api`

Weâ€™ll apply a strict **"deny-all"** policy to the `team-api` namespace.  
This will **block all incoming and outgoing traffic** to/from any pod in that namespace.

---

### ğŸ” NetworkPolicy: `deny-all.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: team-api
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```
kubectl apply -f deny-all.yaml
# Should fail - tester cannot access checkout
kubectl exec -n testers tester -- curl -s checkout.team-api.svc.cluster.local

# Should fail - checkout cannot access internet
kubectl exec -n team-api checkout -- curl -s https://ifconfig.me

### ğŸ”¬ Test After `deny-all`

After applying the `deny-all` network policy, run the following tests to confirm that all ingress and egress traffic is blocked.

| ğŸ” Source          | ğŸ¯ Target          | ğŸ§ª Test Command                                                                 | âœ… Expected Result |
|-------------------|-------------------|--------------------------------------------------------------------------------|--------------------|
| `frontend`        | `checkout`        | `kubectl exec -n team-ui frontend -- curl -s checkout.team-api.svc.cluster.local` | âŒ Blocked         |
| `tester`          | `checkout`        | `kubectl exec -n testers tester -- curl -s checkout.team-api.svc.cluster.local`   | âŒ Blocked         |
| `checkout`        | Internet (Public) | `kubectl exec -n team-api checkout -- curl -s https://ifconfig.me`               | âŒ Blocked         |

ğŸ›¡ï¸ These test results confirm that both **Ingress** and **Egress** traffic have been successfully denied for all Pods in the `team-api` namespace.


## ğŸ§© Phase 4: Use Case 2 â€” Allow Frontend to Access Checkout

### ğŸ¯ NetworkPolicy: `allow-frontend-checkout.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-checkout
  namespace: team-api
spec:
  podSelector:
    matchLabels:
      role: checkout
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: team-ui
    - podSelector:
        matchLabels:
          role: frontend
  policyTypes:
  - Ingress
```
```bash
kubectl apply -f allow-frontend-checkout.yaml
```

### ğŸ”¬ Test After Adding Ingress Policy

| ğŸ” Source   | ğŸ¯ Target   | ğŸ§ª Test Command                                                                 | âœ… Expected Result |
|------------|------------|----------------------------------------------------------------------------------|--------------------|
| frontend   | checkout   | `kubectl exec -n team-ui frontend -- curl -s checkout.team-api.svc.cluster.local` | âœ… Allowed          |
| tester     | checkout   | `kubectl exec -n testers tester -- curl -s checkout.team-api.svc.cluster.local`   | âŒ Blocked          |
| checkout   | Internet   | `kubectl exec -n team-api checkout -- curl -s https://ifconfig.me`                | âŒ Blocked          |


## ğŸŒ Phase 5: Use Case 3 â€” Allow `checkout` â†’ Internet (Egress)

### ğŸŒ NetworkPolicy: `allow-egress-internet.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-internet
  namespace: team-api
spec:
  podSelector:
    matchLabels:
      role: checkout
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
  policyTypes:
  - Egress
```
```bash
kubectl apply -f allow-egress-internet.yaml
```
## ğŸ”¬ Final Test Summary

| ğŸ” Source     | ğŸ¯ Target   | âœ… Expected Result |
|--------------|-------------|--------------------|
| frontend     | checkout    | âœ… Allowed          |
| tester       | checkout    | âŒ Blocked          |
| checkout     | Internet    | âœ… Allowed          |
| inventory    | net         | âŒ Blocked          |

```bash
kubectl delete ns team-api team-ui testers
```


## ğŸ” Use Case: Isolate Pod A from Pod B

### ğŸ¯ Objective
We have two pods in the same namespace:

- `pod-a` with label: `app: a`
- `pod-b` with label: `app: b`

We want:

- âŒ `pod-a` **must not** talk to `pod-b`
- âœ… Other pods (e.g., `frontend`, `tester`) **can** talk to `pod-b`

---

### âš ï¸ Challenge with Kubernetes NetworkPolicy
- Kubernetes NetworkPolicy doesn't support **explicit denies** (no `deny` keyword).
- It follows **additive allow-listing logic**:
  - You allow traffic by defining who **can** talk.
  - Anything not explicitly allowed is **denied**.
- We must write a policy that allows **only selected pods**, excluding `pod-a`.

---

## âœ… Solution: Allow Only Specific Pods to Talk to Pod B

### ğŸ§¾ allow-from-not-a.yaml

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-not-a-to-b
  namespace: team-api
spec:
  podSelector:
    matchLabels:
      app: b  # Target is pod-b
  ingress:
  - from:
    - podSelector:
        matchLabels:
          allow-access: true
  policyTypes:
  - Ingress
```
