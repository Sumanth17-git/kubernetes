## âš ï¸ Pod STATUS FIELDS You Might See

Understanding what each pod status means helps you troubleshoot and debug your workloads.
![image](https://github.com/user-attachments/assets/a44ba842-7f7d-4252-adf5-d85082997d73)

### ğŸ“Š Quick Reference Table

| **Pod STATUS**        | **Meaning**                                                |
|------------------------|------------------------------------------------------------|
| `Running`              | At least one container is running                          |
| `Pending`              | Waiting to be scheduled or pulled                          |
| `Completed` / `Succeeded` | All containers exited successfully                    |
| `Error`                | Container failed with non-zero exit                        |
| `CrashLoopBackOff`     | Failing repeatedly, Kubernetes throttles restart attempts  |
| `ImagePullBackOff`     | Can't pull the image (e.g., registry error, 404)           |
| `ContainerCreating`    | Volume/image/network setup ongoing                         |
| `Terminating`          | Pod is being deleted                                       |

---

### ğŸ§  Detailed Phase Explanations

#### 1ï¸âƒ£ `Pending`

Whatâ€™s Happening:
- âœ… You submit a pod manifest using `kubectl apply -f pod.yaml`
- âœ… Kube API Server accepts it and writes to `etcd` (the Kubernetes database)
- ğŸ”„ Scheduler looks for available nodes:
  - Enough CPU/memory
  - No taint conflicts
  - Affinity/anti-affinity rules satisfied
- ğŸ“Œ Pod status = **Pending** if it hasnâ€™t been scheduled or the container image hasn't been pulled yet.

---

#### 2ï¸âƒ£ `Running`

Whatâ€™s Happening:
- âœ… Scheduler assigns the Pod to a Node
- âœ… Kubelet on that Node:
  - Pulls the image using container runtime (e.g., `containerd`)
  - Mounts volumes (if any)
  - Sets up networking (pod sandbox)
  - Starts the container(s)
- ğŸ¯ Once at least one container starts successfully, status = **Running**

---

#### 3ï¸âƒ£ `Succeeded` (a.k.a. `Completed`)

Whatâ€™s Happening:
- âœ… All containers in the pod **exit with code 0**
- âœ… Kubelet marks pod phase as `Succeeded`
- ğŸ’¡ Common with **Jobs** or **CronJobs**, e.g., data processing task that exits cleanly

---

> ğŸ’¡ **Pro Tip:** Use `kubectl describe pod <pod-name>` to see lifecycle events and root causes of stuck or failed pods.
## ğŸ’¥ What Happens When a Liveness Probe Fails in Kubernetes?

A **liveness probe** checks whether your application inside the container is *still alive*.  
If it fails repeatedly, Kubernetes restarts the container â€” not the entire Pod.

---

### ğŸ”„ Liveness Probe Failure Flow

1ï¸âƒ£ **Probe Fails Once**
- Kubernetes waits and retries the check based on `periodSeconds` and `failureThreshold`.

2ï¸âƒ£ **Probe Fails Multiple Times (e.g., 3 failures)**
- Container is marked as **unhealthy**
- Kubernetes **kills** and **restarts** the container inside the Pod
- The Pod **status remains `Running`**
- The container **restart count increases**

> â„¹ï¸ This happens because the **Pod is still considered healthy** by the kubelet â€” only the container inside failed.

---

### ğŸ§ª How to Observe This

```bash
# Apply the Pod with a liveness probe
kubectl apply -f pod.yaml

# Watch the Pod status and restarts
kubectl get pods -w

# Describe the Pod for probe events
kubectl describe pod <pod-name>

# Check logs (if any)
kubectl logs <pod-name>
```
